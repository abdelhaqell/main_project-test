[
    
    {
        "key": "95ac26fb-5256-4391-950a-bc9f72629b4c",
        "severity": "MAJOR",
        "message": "Bind this resource's automounted service account to RBAC or disable automounting.",
        "component": "spring-petclinic:k8s/db.yml",
        "line": 43,
        "type": "VULNERABILITY",
        "rule": "kubernetes:S6865",
        "status": "OPEN",
        "rule_details": {
            "name": "Service account permissions should be restricted",
            "description": "<h4>Noncompliant code example</h4>\n<p>In this example, the service account token is mounted in the pod <code>example-pod</code> by default, but is unnecessary for the pod and its\nservice(s) to function correctly.</p>\n<pre data-diff-id=\"1\" data-diff-type=\"noncompliant\">\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example-pod\nspec: # Noncompliant\n  containers:\n  - name: example-container\n    image: nginx\n</pre>\n<p>In this example, the service account token is mounted in the pod <code>example-pod</code> and is necessary, for example because it allows a\nthird-party service to authenticate with the Kubernetes API. However, no specific permissions are granted to the service account:</p>\n<pre data-diff-id=\"2\" data-diff-type=\"noncompliant\">\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example-pod\nspec:\n  serviceAccountName: example-sa # Noncompliant\n  containers:\n  - name: example-container\n    image: nginx\n</pre>\n<h4>Compliant solution</h4>\n<pre data-diff-id=\"1\" data-diff-type=\"compliant\">\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example-pod\nspec:\n  containers:\n  - name: example-container\n    image: nginx\n  automountServiceAccountToken: false\n</pre>\n<p>In the following example, Role bindings are created, but Cluster Role Bindings would be more appropriate if the service account is intended to be\nused across multiple namespaces:</p>\n<pre data-diff-id=\"2\" data-diff-type=\"compliant\">\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: example-sa\n  namespace: default\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: default\n  name: example-role\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"list\"]\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: example-role-binding\n  namespace: default\nsubjects:\n- kind: ServiceAccount\n  name: example-sa\n  namespace: default\nroleRef:\n  kind: Role\n  name: example-role\n  apiGroup: rbac.authorization.k8s.io\n\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example-pod\n  namespace: default\nspec:\n  serviceAccountName: example-sa\n  containers:\n  - name: example-container\n    image: nginx\n</pre>\n<h3>How does this work?</h3>\n<p>The essential part of the solution is to make sure that permissions within the cluster are constructed in a way that minimizes the risk of\nunauthorized access.</p>\n<p>To do so, it follows a least-privilege approach.</p>\n<ol>\n  <li> If the service account token is unnecessary for the pod to function, disable automounting. </li>\n  <li> If the service account token is required, ensure that the service account has the least amount of permissions necessary to perform its\n  function. </li>\n</ol>\n<p>Additionally, service account token automounting can be disabled directly from the service account specification file.</p>\n<p>Service account tokens are Kubernetes secrets to authenticate applications running inside pods to the API server. If a pod is compromised, an\nattacker could use this token to gain access to other resources in the cluster.</p>\n<p>For example, they could create new pods, modify existing ones, or even delete critical system pods, depending on the permissions associated with\nthe service account.</p>\n<h3>What is the potential impact?</h3>\n<h4>Unauthorized Access</h4>\n<p>If a pod with a mounted service account gets compromised, an attacker could potentially use the token to interact with the Kubernetes API, possibly\nleading to unauthorized access to other resources in the cluster.</p>\n<h4>Privilege Escalation</h4>\n<p>Service account tokens are often bound with roles that have extensive permissions. If these tokens are exposed, it could lead to privilege\nescalation where an attacker gains higher-level permissions than intended.</p>\n<h4>Data Breach</h4>\n<p>Service account tokens can be used to access sensitive data stored in the Kubernetes cluster. If these tokens are compromised, it could lead to a\ndata breach.</p>\n<h4>Denial of Service</h4>\n<p>An attacker with access to a service account token could potentially overload the Kubernetes API server by sending a large number of requests,\nleading to a Denial of Service (DoS) attack.</p>\n<h3>Documentation</h3>\n<ul>\n  <li> Kubernetes Documentation - <a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\">Configure Service\n  Accounts for Pods</a> </li>\n</ul>\n<h3>Standards</h3>\n<ul>\n  <li> CWE - <a href=\"https://cwe.mitre.org/data/definitions/306\">CWE-306 - Missing Authentication for Critical Function</a> </li>\n</ul>",
            "rule_severity": "MAJOR",
            "rule_type": "VULNERABILITY",
            "rule_status": "READY"
        }
    },
    {
        "key": "3d5ec6ce-e4e7-4d8a-92df-6b69e9d008a5",
        "severity": "MAJOR",
        "message": "Specify a storage request for this container.",
        "component": "spring-petclinic:k8s/db.yml",
        "line": 44,
        "type": "CODE_SMELL",
        "rule": "kubernetes:S6897",
        "status": "OPEN",
        "rule_details": {
            "name": "Storage requests should be specified",
            "description": "<p>To avoid potential issues, specify a storage request for each container using ephemeral storage with\n<code>resources.requests.ephemeral-storage</code>, or create a <code>LimitRange</code> resource, that sets a default storage request for all\ncontainers in all pod specifications belonging to the same namespace.</p>\n<h4>Noncompliant code example</h4>\n<pre data-diff-id=\"1\" data-diff-type=\"noncompliant\">\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\nspec:\n  containers:\n    - name: web # Noncompliant\n      image: nginx\n      volumeMounts:\n        - name: ephemeral\n          mountPath: \"/tmp\"\n</pre>\n<pre data-diff-id=\"2\" data-diff-type=\"noncompliant\">\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\nspec:\n  containers:\n    - name: web # Noncompliant\n      image: nginx\n      volumeMounts:\n        - name: ephemeral\n          mountPath: \"/tmp\"\n</pre>\n<h4>Compliant solution</h4>\n<pre data-diff-id=\"1\" data-diff-type=\"compliant\">\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\nspec:\n  containers:\n    - name: web\n      image: nginx\n      resources:\n        requests:\n          ephemeral-storage: \"2Gi\"\n      volumeMounts:\n        - name: ephemeral\n          mountPath: \"/tmp\"\n</pre>\n<pre data-diff-id=\"2\" data-diff-type=\"compliant\">\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: storage-limit-range\n  namespace: namespace-with-limit-range\nspec:\n  limits:\n  - defaultRequest:\n      ephemeral-storage: \"10Mi\"\n    type: Container\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\n  namespace: namespace-with-limit-range\nspec:\n  containers:\n    - name: web\n      image: nginx\n      volumeMounts:\n        - name: ephemeral\n          mountPath: \"/tmp\"\n</pre>\n<h3>How does this work?</h3>\n<p>You can set a request through the property <code>resources.requests.ephemeral-storage</code> of a container. Alternatively, you can set a default\nrequest for a namespace with <code>LimitRange</code> through <code>spec.limits[].defaultRequest.ephemeral-storage</code>.</p>\n<p>Ephemeral storage is a type of storage that is temporary and non-persistent, meaning it does not retain data once the process is terminated. In the\ncontext of Kubernetes, ephemeral storage is used for storing temporary files that a running container can write and read.</p>\n<h3>Documentation</h3>\n<ul>\n  <li> Kubernetes Documentation - <a\n  href=\"https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#setting-requests-and-limits-for-local-ephemeral-storage\">Setting requests and limits for local ephemeral storage</a> </li>\n</ul>\n<p>Without a storage request, a container can potentially be scheduled on a node where there are not enough resources for it. This can lead to\nunpredictable behavior of the container and the node itself.</p>\n<h3>What is the potential impact?</h3>\n<h4>Unpredictable Resource Allocation</h4>\n<p>Kubernetes doesn’t know how much of a particular resource to allocate to a container without defined requests. This can lead to unpredictable\nbehavior, as the Kubernetes scheduler may not make optimal decisions about pod placement and resource contention management. For instance, a container\nmight not get the resources it needs to function correctly, leading to performance issues or even failure of the container.</p>\n<h4>System Instability</h4>\n<p>In the worst-case scenario, if a container uses more resources than a node can handle (due to lack of defined requests), it can cause the node to\nrun out of resources. This can lead to system instability, and in extreme cases, the node might crash, causing downtime for all containers running on\nthat node.</p>",
            "rule_severity": "MAJOR",
            "rule_type": "CODE_SMELL",
            "rule_status": "READY"
        }
    },
    {
        "key": "be91d223-55f8-4cd6-8919-894e0297b601",
        "severity": "MAJOR",
        "message": "Specify a memory request for this container.",
        "component": "spring-petclinic:k8s/db.yml",
        "line": 44,
        "type": "CODE_SMELL",
        "rule": "kubernetes:S6873",
        "status": "OPEN",
        "rule_details": {
            "name": "Memory requests should be specified",
            "description": "<p>A memory request is a configuration that sets the guaranteed amount of memory that a container will be able to use. It is part of the resource\nmanagement functionality of Kubernetes, which allows for the control and allocation of computational resources to containers.</p>\n<p>When a memory request is set for a container, Kubernetes will only schedule it on a node that can give it that resource, thereby guaranteeing that\nthe container can use the specified requested memory.</p>\n<p>Without a memory request, a container can potentially be scheduled on a node where there are not enough resources for it. This can lead to\nunpredictable behavior of the container and the node itself.</p>\n<h3>What is the potential impact?</h3>\n<h4>Unpredictable Resource Allocation</h4>\n<p>Without defined requests, Kubernetes doesn’t know how much of a particular resource to allocate to a container. This can lead to unpredictable\nbehavior, as the Kubernetes scheduler may not make optimal decisions about pod placement and resource contention management. For instance, a container\nmight not get the resources it needs to function correctly, leading to performance issues or even failure of the container.</p>\n<h4>System Instability</h4>\n<p>In the worst-case scenario, if a container uses more resources than a node can handle (due to lack of defined requests), it can cause the node to\nrun out of resources. This can lead to system instability, and in extreme cases, the node might crash, causing downtime for all containers running on\nthat node.</p>\n<h3>Documentation</h3>\n<ul>\n  <li> Kubernetes Documentation - <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/\">Configure\n  Default Memory Requests and Limits for a Namespace</a> </li>\n</ul>\n<h3>Articles &amp; blog posts</h3>\n<ul>\n  <li> Google Cloud Blog - <a\n  href=\"https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-resource-requests-and-limits\">Kubernetes best\n  practices: Resource requests and limits</a> </li>\n</ul>\n<p>To avoid potential issues, either specify a memory request for each container in a pod specification or create a resource of a kind,\n<code>LimitRange</code>, that sets a default memory request for all containers in all pod specifications belonging to the same namespace.</p>\n<h4>Noncompliant code example</h4>\n<pre data-diff-id=\"1\" data-diff-type=\"noncompliant\">\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\nspec:\n  containers:\n    - name: web # Noncompliant\n      image: nginx\n</pre>\n<pre data-diff-id=\"2\" data-diff-type=\"noncompliant\">\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\nspec:\n  containers:\n    - name: web # Noncompliant\n      image: nginx\n</pre>\n<h4>Compliant solution</h4>\n<pre data-diff-id=\"1\" data-diff-type=\"compliant\">\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\nspec:\n  containers:\n    - name: web\n      image: nginx\n      resources:\n        requests:\n          memory: 100Mi\n</pre>\n<pre data-diff-id=\"2\" data-diff-type=\"compliant\">\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: mem-limit-range\n  namespace: default-mem-example\nspec:\n  limits:\n    - type: Container\n      defaultRequest:\n        memory: 100Mi\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\n  namespace: default-mem-example\nspec:\n  containers:\n    - name: web\n      image: nginx\n</pre>\n<h3>How does this work?</h3>\n<p>A request can be set through the property <code>resources.requests.memory</code> of a container. Alternatively, a default request for a namespace\ncan be set with <code>LimitRange</code> through <code>spec.limits[].defaultRequest.memory</code>.</p>",
            "rule_severity": "MAJOR",
            "rule_type": "CODE_SMELL",
            "rule_status": "READY"
        }
    },
    {
        "key": "db310c48-5646-4d40-884e-417de7a209d4",
        "severity": "MAJOR",
        "message": "Specify a CPU request for this container.",
        "component": "spring-petclinic:k8s/db.yml",
        "line": 44,
        "type": "CODE_SMELL",
        "rule": "kubernetes:S6892",
        "status": "OPEN",
        "rule_details": {
            "name": "CPU requests should be specified",
            "description": "<p>Without a CPU request, a container can potentially be scheduled on a node where there are not enough resources for it. This can lead to\nunpredictable behavior of the container and the node itself.</p>\n<h3>What is the potential impact?</h3>\n<h4>Unpredictable Resource Allocation</h4>\n<p>Without defined requests, Kubernetes doesn’t know how much of a particular resource to allocate to a container. This can lead to unpredictable\nbehavior, as the Kubernetes scheduler may not make optimal decisions about pod placement and resource contention management. For instance, a container\nmight not get the resources it needs to function correctly, leading to performance issues or even failure of the container.</p>\n<h4>System Instability</h4>\n<p>In the worst-case scenario, if a container uses more resources than a node can handle (due to lack of defined requests), it can cause the node to\nrun out of resources. In this case, Kubernetes may throttle its CPU usage. By setting a CPU request, Kubernetes will make sure that the container will\nget the requested CPU.</p>\n<p>To avoid potential issues, either specify a CPU request for each container with <code>resources.requests.cpu</code> or create a resource of a kind\n<code>LimitRange</code> that sets a default CPU request for all containers in all pod specifications in a namespace.</p>\n<h4>Noncompliant code example</h4>\n<pre data-diff-id=\"1\" data-diff-type=\"noncompliant\">\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\nspec:\n  containers:\n    - name: web # Noncompliant\n      image: nginx\n</pre>\n<pre data-diff-id=\"2\" data-diff-type=\"noncompliant\">\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\nspec:\n  containers:\n    - name: web # Noncompliant\n      image: nginx\n</pre>\n<h4>Compliant solution</h4>\n<pre data-diff-id=\"1\" data-diff-type=\"compliant\">\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\nspec:\n  containers:\n    - name: web\n      image: nginx\n      resources:\n        requests:\n          cpu: 0.5\n</pre>\n<pre data-diff-id=\"2\" data-diff-type=\"compliant\">\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: cpu-request-range\n  namespace: default-cpu-example\nspec:\n  limits:\n  - defaultRequest:\n      cpu: 0.5\n    type: Container\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-ns-compliant\n  namespace: default-cpu-example\nspec:\n  containers:\n  - name: nginx-ns-compliant\n    image: nginx\n</pre>\n<h3>How does this work?</h3>\n<p>A request can be set through the property <code>resources.requests.cpu</code> of a container. Alternatively, a default request for a namespace can\nbe set with <code>LimitRange</code> through the property <code>spec.limits[].defaultRequest.cpu</code>.</p>\n<h3>Documentation</h3>\n<ul>\n  <li> Kubernetes Documentation - <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/\">Configure\n  Default CPU Requests and Limits for a Namespace</a> </li>\n</ul>\n<h3>Articles &amp; blog posts</h3>\n<ul>\n  <li> Google Cloud Blog - <a\n  href=\"https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-resource-requests-and-limits\">Kubernetes best\n  practices: Resource requests and limits</a> </li>\n</ul>\n<p>A CPU request is a configuration that sets the guaranteed amount of CPU cores that a container will be able to use. It is part of the resource\nmanagement functionality of Kubernetes, which allows for the control and allocation of computational resources to containers.</p>\n<p>When a CPU request is set for a container, Kubernetes will only schedule it on a node that can give it that resource, thereby guaranteeing that the\ncontainer can use the specified requested CPU cores.</p>",
            "rule_severity": "MAJOR",
            "rule_type": "CODE_SMELL",
            "rule_status": "READY"
        }
    },
    {
        "key": "f83e8194-d89f-4f2a-b49a-50e3338b48e7",
        "severity": "MAJOR",
        "message": "Specify a memory limit for this container.",
        "component": "spring-petclinic:k8s/db.yml",
        "line": 44,
        "type": "VULNERABILITY",
        "rule": "kubernetes:S6864",
        "status": "OPEN",
        "rule_details": {
            "name": "Memory limits should be enforced",
            "description": "<h3>Documentation</h3>\n<ul>\n  <li> Kubernetes Documentation - <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/\">Configure\n  Default Memory Requests and Limits for a Namespace</a> </li>\n</ul>\n<h3>Standards</h3>\n<ul>\n  <li> CWE - <a href=\"https://cwe.mitre.org/data/definitions/770\">CWE-770 - Allocation of Resources Without Limits or Throttling</a> </li>\n</ul>\n<p>To avoid potential issues, either specify a memory limit for each container in a pod specification or create a resource of a kind\n<code>LimitRange</code>, that sets a default memory limit for all containers in all pod specifications belonging to the same namespace.</p>\n<h4>Noncompliant code example</h4>\n<pre data-diff-id=\"1\" data-diff-type=\"noncompliant\">\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\nspec:\n  containers:\n    - name: web # Noncompliant\n      image: nginx\n</pre>\n<pre data-diff-id=\"2\" data-diff-type=\"noncompliant\">\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\nspec:\n  containers:\n    - name: web # Noncompliant\n      image: nginx\n</pre>\n<h4>Compliant solution</h4>\n<pre data-diff-id=\"1\" data-diff-type=\"compliant\">\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\nspec:\n  containers:\n    - name: web\n      image: nginx\n      resources:\n        limits:\n          memory: 100Mi\n</pre>\n<pre data-diff-id=\"2\" data-diff-type=\"compliant\">\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: mem-limit-range\n  namespace: default-mem-example\nspec:\n  limits:\n    - type: Container\n      default:\n        memory: 100Mi\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\n  namespace: default-mem-example\nspec:\n  containers:\n    - name: web\n      image: nginx\n</pre>\n<h3>How does this work?</h3>\n<p>A limit can be set through the property <code>resources.limits.memory</code> of a container. Alternatively, a default limit for a namespace can be\nset with <code>LimitRange</code> through <code>spec.limits[].default.memory</code>.</p>\n<p>A memory limit is a configuration that sets the maximum amount of memory that a container can use. It is part of the resource management\nfunctionality of Kubernetes, which allows for the control and allocation of computational resources to containers.</p>\n<p>When a memory limit is set for a container, Kubernetes ensures that the container does not exceed the specified limit. If a container tries to use\nmore memory than its limit, the system will reclaim the excess memory, which could lead to termination of processes within the container.</p>\n<p>Without a memory limit, a container can potentially consume all available memory on a node, which can lead to unpredictable behavior of the\ncontainer or the node itself. Therefore, defining a memory limit for each container is a best practice in Kubernetes configurations. It helps in\nmanaging resources effectively and ensures that a single container does not monopolize the memory resources of a node.</p>\n<h3>What is the potential impact?</h3>\n<h4>Denial of Service</h4>\n<p>Without a memory limit, a container can consume all available memory on a node. This could lead to a Denial of Service (DoS) condition where other\ncontainers on the same node are starved of memory. These containers may slow down, become unresponsive, or even crash, affecting the overall\nfunctionality and availability of applications running on them.</p>\n<h4>Inefficient Resource Allocation</h4>\n<p>When containers lack specified resource requests, the Kubernetes scheduler may not make optimal decisions about pod placement and resource\ncontention management. This could result in the scheduler placing a resource-intensive pod on a node with insufficient resources, leading to\nperformance issues or even node failure.</p>",
            "rule_severity": "MAJOR",
            "rule_type": "VULNERABILITY",
            "rule_status": "READY"
        }
    }
]