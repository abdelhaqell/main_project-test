[
    
    {
        "key": "12dd5aee-7c0f-4512-8ac1-d8e749434755",
        "severity": "MAJOR",
        "message": "Use a specific version tag for the image.",
        "component": "spring-petclinic:k8s/petclinic.yml",
        "line": null,
        "type": "CODE_SMELL",
        "rule": "kubernetes:S6596",
        "status": "CLOSED",
        "rule_details": {
            "name": "Specific version tag for image should be used",
            "description": "<p>When a container image is not tagged with a specific version, it is referred to as <code>latest</code>. This means that every time the image is\nbuilt, deployed, or run, it will always use the latest version of the image.</p>\n<h3>Documentation</h3>\n<ul>\n  <li> <a href=\"https://helm.sh/docs/chart_best_practices/pods/#images\">Helm chart best practices - Images</a> </li>\n  <li> <a href=\"https://kubernetes.io/docs/concepts/containers/images/\">Kubernetes - Images</a> </li>\n  <li> <a href=\"https://docs.docker.com/engine/reference/commandline/image_pull/#pull-an-image-by-digest-immutable-identifier\">Pull an image by digest\n  (immutable identifier)</a> </li>\n</ul>\n<p>To avoid these issues, it is recommended to use specific version tags for container images.</p>\n<p>This can be done by appending the version number or tag to the container image name. For example, instead of <code>my-image:latest</code>, it is\nbetter to use <code>my-image:1.2.3-alpine</code> or <code>my-image:1.2.3</code>.</p>\n<p>For even more control and traceability, it is also possible to specify your image by digest using the sha256 of the image. This will pin your image\nto a specific version in time, but will also exclude it from eventual security updates. An example would be using\n<code>my-image@sha256:26c68657ccce2cb0a31b330cb0be2b5e108d467f641c62e13ab40cbec258c68d</code>.</p>\n<p>More information can be found in the documentation at the end.</p>\n\n<h4>Noncompliant code example</h4>\n<pre data-diff-id=\"1\" data-diff-type=\"noncompliant\">\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\nspec:\n  containers:\n    - name: nginx\n      image: nginx # Noncompliant\n\n    - name: nginx\n      image: nginx:latest # Noncompliant\n</pre>\n<h4>Compliant solution</h4>\n<pre data-diff-id=\"1\" data-diff-type=\"compliant\">\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\nspec:\n  containers:\n    - name: nginx\n      image: nginx:1.14.2 # Compliant, fixed tag\n\n    - name: nginx\n      image: nginx@sha256:b0ad43f7ee5edbc0effbc14645ae7055e21bc1973aee5150745632a24a752661 # Compliant, SHA of the image\n</pre>\n<h3>How does this work?</h3>\n<p>This way, the same version of the container image is used every time the application is built, deployed, or run, ensuring consistency and\npredictability across different environments. It is also not enough to use the latest tag, as this version also changes with each release.</p>\n<h3>Going the extra mile</h3>\n<p>Adhering to this can also make it easier to track which version of the container image is being used, which can be useful for debugging and\ntroubleshooting purposes.</p>\n<p>While using always the latest version may seem convenient, the build cannot be repeated because it is not clear which was the last version. In\naddition, it can lead to unpredictability and issues such as version mismatch and potential security vulnerabilities.</p>\n<h3>What is the potential impact?</h3>\n<p>For example, if a developer builds and deploys an application using <code>my-image:latest</code>, they may unknowingly be using a different version\nof the image than another developer who also built and deployed the same application using <code>my-image:latest</code>. This can lead to version\nmismatches, which can cause bugs or compatibility issues.</p>\n<p>In addition, using <code>latest</code> as the tag for container images can potentially introduce security vulnerabilities. For instance, if a\nsecurity vulnerability is discovered in an image and a new version is released to fix it, using <code>latest</code> as the tag means that the\napplication will automatically use the updated image, even if it has not been properly tested and vetted for compatibility with the application.</p>",
            "rule_severity": "MAJOR",
            "rule_type": "CODE_SMELL",
            "rule_status": "READY"
        }
    },
    {
        "key": "c881b829-d604-47ea-91c9-5f24edb6ea18",
        "severity": "MAJOR",
        "message": "Bind this resource's automounted service account to RBAC or disable automounting.",
        "component": "spring-petclinic:k8s/petclinic.yml",
        "line": 31,
        "type": "VULNERABILITY",
        "rule": "kubernetes:S6865",
        "status": "OPEN",
        "rule_details": {
            "name": "Service account permissions should be restricted",
            "description": "<h4>Noncompliant code example</h4>\n<p>In this example, the service account token is mounted in the pod <code>example-pod</code> by default, but is unnecessary for the pod and its\nservice(s) to function correctly.</p>\n<pre data-diff-id=\"1\" data-diff-type=\"noncompliant\">\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example-pod\nspec: # Noncompliant\n  containers:\n  - name: example-container\n    image: nginx\n</pre>\n<p>In this example, the service account token is mounted in the pod <code>example-pod</code> and is necessary, for example because it allows a\nthird-party service to authenticate with the Kubernetes API. However, no specific permissions are granted to the service account:</p>\n<pre data-diff-id=\"2\" data-diff-type=\"noncompliant\">\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example-pod\nspec:\n  serviceAccountName: example-sa # Noncompliant\n  containers:\n  - name: example-container\n    image: nginx\n</pre>\n<h4>Compliant solution</h4>\n<pre data-diff-id=\"1\" data-diff-type=\"compliant\">\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example-pod\nspec:\n  containers:\n  - name: example-container\n    image: nginx\n  automountServiceAccountToken: false\n</pre>\n<p>In the following example, Role bindings are created, but Cluster Role Bindings would be more appropriate if the service account is intended to be\nused across multiple namespaces:</p>\n<pre data-diff-id=\"2\" data-diff-type=\"compliant\">\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: example-sa\n  namespace: default\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: default\n  name: example-role\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"list\"]\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: example-role-binding\n  namespace: default\nsubjects:\n- kind: ServiceAccount\n  name: example-sa\n  namespace: default\nroleRef:\n  kind: Role\n  name: example-role\n  apiGroup: rbac.authorization.k8s.io\n\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example-pod\n  namespace: default\nspec:\n  serviceAccountName: example-sa\n  containers:\n  - name: example-container\n    image: nginx\n</pre>\n<h3>How does this work?</h3>\n<p>The essential part of the solution is to make sure that permissions within the cluster are constructed in a way that minimizes the risk of\nunauthorized access.</p>\n<p>To do so, it follows a least-privilege approach.</p>\n<ol>\n  <li> If the service account token is unnecessary for the pod to function, disable automounting. </li>\n  <li> If the service account token is required, ensure that the service account has the least amount of permissions necessary to perform its\n  function. </li>\n</ol>\n<p>Additionally, service account token automounting can be disabled directly from the service account specification file.</p>\n<p>Service account tokens are Kubernetes secrets to authenticate applications running inside pods to the API server. If a pod is compromised, an\nattacker could use this token to gain access to other resources in the cluster.</p>\n<p>For example, they could create new pods, modify existing ones, or even delete critical system pods, depending on the permissions associated with\nthe service account.</p>\n<h3>What is the potential impact?</h3>\n<h4>Unauthorized Access</h4>\n<p>If a pod with a mounted service account gets compromised, an attacker could potentially use the token to interact with the Kubernetes API, possibly\nleading to unauthorized access to other resources in the cluster.</p>\n<h4>Privilege Escalation</h4>\n<p>Service account tokens are often bound with roles that have extensive permissions. If these tokens are exposed, it could lead to privilege\nescalation where an attacker gains higher-level permissions than intended.</p>\n<h4>Data Breach</h4>\n<p>Service account tokens can be used to access sensitive data stored in the Kubernetes cluster. If these tokens are compromised, it could lead to a\ndata breach.</p>\n<h4>Denial of Service</h4>\n<p>An attacker with access to a service account token could potentially overload the Kubernetes API server by sending a large number of requests,\nleading to a Denial of Service (DoS) attack.</p>\n<h3>Documentation</h3>\n<ul>\n  <li> Kubernetes Documentation - <a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\">Configure Service\n  Accounts for Pods</a> </li>\n</ul>\n<h3>Standards</h3>\n<ul>\n  <li> CWE - <a href=\"https://cwe.mitre.org/data/definitions/306\">CWE-306 - Missing Authentication for Critical Function</a> </li>\n</ul>",
            "rule_severity": "MAJOR",
            "rule_type": "VULNERABILITY",
            "rule_status": "READY"
        }
    },
    {
        "key": "12b78e4b-53f5-40ec-a7a4-17290077f2d4",
        "severity": "MAJOR",
        "message": "Specify a memory request for this container.",
        "component": "spring-petclinic:k8s/petclinic.yml",
        "line": 32,
        "type": "CODE_SMELL",
        "rule": "kubernetes:S6873",
        "status": "OPEN",
        "rule_details": {
            "name": "Memory requests should be specified",
            "description": "<p>A memory request is a configuration that sets the guaranteed amount of memory that a container will be able to use. It is part of the resource\nmanagement functionality of Kubernetes, which allows for the control and allocation of computational resources to containers.</p>\n<p>When a memory request is set for a container, Kubernetes will only schedule it on a node that can give it that resource, thereby guaranteeing that\nthe container can use the specified requested memory.</p>\n<p>Without a memory request, a container can potentially be scheduled on a node where there are not enough resources for it. This can lead to\nunpredictable behavior of the container and the node itself.</p>\n<h3>What is the potential impact?</h3>\n<h4>Unpredictable Resource Allocation</h4>\n<p>Without defined requests, Kubernetes doesn’t know how much of a particular resource to allocate to a container. This can lead to unpredictable\nbehavior, as the Kubernetes scheduler may not make optimal decisions about pod placement and resource contention management. For instance, a container\nmight not get the resources it needs to function correctly, leading to performance issues or even failure of the container.</p>\n<h4>System Instability</h4>\n<p>In the worst-case scenario, if a container uses more resources than a node can handle (due to lack of defined requests), it can cause the node to\nrun out of resources. This can lead to system instability, and in extreme cases, the node might crash, causing downtime for all containers running on\nthat node.</p>\n<h3>Documentation</h3>\n<ul>\n  <li> Kubernetes Documentation - <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/\">Configure\n  Default Memory Requests and Limits for a Namespace</a> </li>\n</ul>\n<h3>Articles &amp; blog posts</h3>\n<ul>\n  <li> Google Cloud Blog - <a\n  href=\"https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-resource-requests-and-limits\">Kubernetes best\n  practices: Resource requests and limits</a> </li>\n</ul>\n<p>To avoid potential issues, either specify a memory request for each container in a pod specification or create a resource of a kind,\n<code>LimitRange</code>, that sets a default memory request for all containers in all pod specifications belonging to the same namespace.</p>\n<h4>Noncompliant code example</h4>\n<pre data-diff-id=\"1\" data-diff-type=\"noncompliant\">\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\nspec:\n  containers:\n    - name: web # Noncompliant\n      image: nginx\n</pre>\n<pre data-diff-id=\"2\" data-diff-type=\"noncompliant\">\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\nspec:\n  containers:\n    - name: web # Noncompliant\n      image: nginx\n</pre>\n<h4>Compliant solution</h4>\n<pre data-diff-id=\"1\" data-diff-type=\"compliant\">\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\nspec:\n  containers:\n    - name: web\n      image: nginx\n      resources:\n        requests:\n          memory: 100Mi\n</pre>\n<pre data-diff-id=\"2\" data-diff-type=\"compliant\">\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: mem-limit-range\n  namespace: default-mem-example\nspec:\n  limits:\n    - type: Container\n      defaultRequest:\n        memory: 100Mi\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\n  namespace: default-mem-example\nspec:\n  containers:\n    - name: web\n      image: nginx\n</pre>\n<h3>How does this work?</h3>\n<p>A request can be set through the property <code>resources.requests.memory</code> of a container. Alternatively, a default request for a namespace\ncan be set with <code>LimitRange</code> through <code>spec.limits[].defaultRequest.memory</code>.</p>",
            "rule_severity": "MAJOR",
            "rule_type": "CODE_SMELL",
            "rule_status": "READY"
        }
    },
    {
        "key": "12f8c6c1-32ab-428d-afcc-f35a39f6b085",
        "severity": "MAJOR",
        "message": "Specify a memory limit for this container.",
        "component": "spring-petclinic:k8s/petclinic.yml",
        "line": 32,
        "type": "VULNERABILITY",
        "rule": "kubernetes:S6864",
        "status": "OPEN",
        "rule_details": {
            "name": "Memory limits should be enforced",
            "description": "<h3>Documentation</h3>\n<ul>\n  <li> Kubernetes Documentation - <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/\">Configure\n  Default Memory Requests and Limits for a Namespace</a> </li>\n</ul>\n<h3>Standards</h3>\n<ul>\n  <li> CWE - <a href=\"https://cwe.mitre.org/data/definitions/770\">CWE-770 - Allocation of Resources Without Limits or Throttling</a> </li>\n</ul>\n<p>To avoid potential issues, either specify a memory limit for each container in a pod specification or create a resource of a kind\n<code>LimitRange</code>, that sets a default memory limit for all containers in all pod specifications belonging to the same namespace.</p>\n<h4>Noncompliant code example</h4>\n<pre data-diff-id=\"1\" data-diff-type=\"noncompliant\">\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\nspec:\n  containers:\n    - name: web # Noncompliant\n      image: nginx\n</pre>\n<pre data-diff-id=\"2\" data-diff-type=\"noncompliant\">\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\nspec:\n  containers:\n    - name: web # Noncompliant\n      image: nginx\n</pre>\n<h4>Compliant solution</h4>\n<pre data-diff-id=\"1\" data-diff-type=\"compliant\">\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\nspec:\n  containers:\n    - name: web\n      image: nginx\n      resources:\n        limits:\n          memory: 100Mi\n</pre>\n<pre data-diff-id=\"2\" data-diff-type=\"compliant\">\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: mem-limit-range\n  namespace: default-mem-example\nspec:\n  limits:\n    - type: Container\n      default:\n        memory: 100Mi\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\n  namespace: default-mem-example\nspec:\n  containers:\n    - name: web\n      image: nginx\n</pre>\n<h3>How does this work?</h3>\n<p>A limit can be set through the property <code>resources.limits.memory</code> of a container. Alternatively, a default limit for a namespace can be\nset with <code>LimitRange</code> through <code>spec.limits[].default.memory</code>.</p>\n<p>A memory limit is a configuration that sets the maximum amount of memory that a container can use. It is part of the resource management\nfunctionality of Kubernetes, which allows for the control and allocation of computational resources to containers.</p>\n<p>When a memory limit is set for a container, Kubernetes ensures that the container does not exceed the specified limit. If a container tries to use\nmore memory than its limit, the system will reclaim the excess memory, which could lead to termination of processes within the container.</p>\n<p>Without a memory limit, a container can potentially consume all available memory on a node, which can lead to unpredictable behavior of the\ncontainer or the node itself. Therefore, defining a memory limit for each container is a best practice in Kubernetes configurations. It helps in\nmanaging resources effectively and ensures that a single container does not monopolize the memory resources of a node.</p>\n<h3>What is the potential impact?</h3>\n<h4>Denial of Service</h4>\n<p>Without a memory limit, a container can consume all available memory on a node. This could lead to a Denial of Service (DoS) condition where other\ncontainers on the same node are starved of memory. These containers may slow down, become unresponsive, or even crash, affecting the overall\nfunctionality and availability of applications running on them.</p>\n<h4>Inefficient Resource Allocation</h4>\n<p>When containers lack specified resource requests, the Kubernetes scheduler may not make optimal decisions about pod placement and resource\ncontention management. This could result in the scheduler placing a resource-intensive pod on a node with insufficient resources, leading to\nperformance issues or even node failure.</p>",
            "rule_severity": "MAJOR",
            "rule_type": "VULNERABILITY",
            "rule_status": "READY"
        }
    },
    {
        "key": "5ba4e55e-c0b7-4cea-aed3-d5fb48f653a0",
        "severity": "MAJOR",
        "message": "Specify a storage request for this container.",
        "component": "spring-petclinic:k8s/petclinic.yml",
        "line": 32,
        "type": "CODE_SMELL",
        "rule": "kubernetes:S6897",
        "status": "OPEN",
        "rule_details": {
            "name": "Storage requests should be specified",
            "description": "<p>To avoid potential issues, specify a storage request for each container using ephemeral storage with\n<code>resources.requests.ephemeral-storage</code>, or create a <code>LimitRange</code> resource, that sets a default storage request for all\ncontainers in all pod specifications belonging to the same namespace.</p>\n<h4>Noncompliant code example</h4>\n<pre data-diff-id=\"1\" data-diff-type=\"noncompliant\">\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\nspec:\n  containers:\n    - name: web # Noncompliant\n      image: nginx\n      volumeMounts:\n        - name: ephemeral\n          mountPath: \"/tmp\"\n</pre>\n<pre data-diff-id=\"2\" data-diff-type=\"noncompliant\">\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\nspec:\n  containers:\n    - name: web # Noncompliant\n      image: nginx\n      volumeMounts:\n        - name: ephemeral\n          mountPath: \"/tmp\"\n</pre>\n<h4>Compliant solution</h4>\n<pre data-diff-id=\"1\" data-diff-type=\"compliant\">\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\nspec:\n  containers:\n    - name: web\n      image: nginx\n      resources:\n        requests:\n          ephemeral-storage: \"2Gi\"\n      volumeMounts:\n        - name: ephemeral\n          mountPath: \"/tmp\"\n</pre>\n<pre data-diff-id=\"2\" data-diff-type=\"compliant\">\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: storage-limit-range\n  namespace: namespace-with-limit-range\nspec:\n  limits:\n  - defaultRequest:\n      ephemeral-storage: \"10Mi\"\n    type: Container\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\n  namespace: namespace-with-limit-range\nspec:\n  containers:\n    - name: web\n      image: nginx\n      volumeMounts:\n        - name: ephemeral\n          mountPath: \"/tmp\"\n</pre>\n<h3>How does this work?</h3>\n<p>You can set a request through the property <code>resources.requests.ephemeral-storage</code> of a container. Alternatively, you can set a default\nrequest for a namespace with <code>LimitRange</code> through <code>spec.limits[].defaultRequest.ephemeral-storage</code>.</p>\n<p>Ephemeral storage is a type of storage that is temporary and non-persistent, meaning it does not retain data once the process is terminated. In the\ncontext of Kubernetes, ephemeral storage is used for storing temporary files that a running container can write and read.</p>\n<h3>Documentation</h3>\n<ul>\n  <li> Kubernetes Documentation - <a\n  href=\"https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#setting-requests-and-limits-for-local-ephemeral-storage\">Setting requests and limits for local ephemeral storage</a> </li>\n</ul>\n<p>Without a storage request, a container can potentially be scheduled on a node where there are not enough resources for it. This can lead to\nunpredictable behavior of the container and the node itself.</p>\n<h3>What is the potential impact?</h3>\n<h4>Unpredictable Resource Allocation</h4>\n<p>Kubernetes doesn’t know how much of a particular resource to allocate to a container without defined requests. This can lead to unpredictable\nbehavior, as the Kubernetes scheduler may not make optimal decisions about pod placement and resource contention management. For instance, a container\nmight not get the resources it needs to function correctly, leading to performance issues or even failure of the container.</p>\n<h4>System Instability</h4>\n<p>In the worst-case scenario, if a container uses more resources than a node can handle (due to lack of defined requests), it can cause the node to\nrun out of resources. This can lead to system instability, and in extreme cases, the node might crash, causing downtime for all containers running on\nthat node.</p>",
            "rule_severity": "MAJOR",
            "rule_type": "CODE_SMELL",
            "rule_status": "READY"
        }
    },
    {
        "key": "d9dce8a5-84c8-4fb8-8fd8-41c7bddd40ec",
        "severity": "MAJOR",
        "message": "Specify a CPU request for this container.",
        "component": "spring-petclinic:k8s/petclinic.yml",
        "line": 32,
        "type": "CODE_SMELL",
        "rule": "kubernetes:S6892",
        "status": "OPEN",
        "rule_details": {
            "name": "CPU requests should be specified",
            "description": "<p>Without a CPU request, a container can potentially be scheduled on a node where there are not enough resources for it. This can lead to\nunpredictable behavior of the container and the node itself.</p>\n<h3>What is the potential impact?</h3>\n<h4>Unpredictable Resource Allocation</h4>\n<p>Without defined requests, Kubernetes doesn’t know how much of a particular resource to allocate to a container. This can lead to unpredictable\nbehavior, as the Kubernetes scheduler may not make optimal decisions about pod placement and resource contention management. For instance, a container\nmight not get the resources it needs to function correctly, leading to performance issues or even failure of the container.</p>\n<h4>System Instability</h4>\n<p>In the worst-case scenario, if a container uses more resources than a node can handle (due to lack of defined requests), it can cause the node to\nrun out of resources. In this case, Kubernetes may throttle its CPU usage. By setting a CPU request, Kubernetes will make sure that the container will\nget the requested CPU.</p>\n<p>To avoid potential issues, either specify a CPU request for each container with <code>resources.requests.cpu</code> or create a resource of a kind\n<code>LimitRange</code> that sets a default CPU request for all containers in all pod specifications in a namespace.</p>\n<h4>Noncompliant code example</h4>\n<pre data-diff-id=\"1\" data-diff-type=\"noncompliant\">\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\nspec:\n  containers:\n    - name: web # Noncompliant\n      image: nginx\n</pre>\n<pre data-diff-id=\"2\" data-diff-type=\"noncompliant\">\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\nspec:\n  containers:\n    - name: web # Noncompliant\n      image: nginx\n</pre>\n<h4>Compliant solution</h4>\n<pre data-diff-id=\"1\" data-diff-type=\"compliant\">\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\nspec:\n  containers:\n    - name: web\n      image: nginx\n      resources:\n        requests:\n          cpu: 0.5\n</pre>\n<pre data-diff-id=\"2\" data-diff-type=\"compliant\">\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: cpu-request-range\n  namespace: default-cpu-example\nspec:\n  limits:\n  - defaultRequest:\n      cpu: 0.5\n    type: Container\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-ns-compliant\n  namespace: default-cpu-example\nspec:\n  containers:\n  - name: nginx-ns-compliant\n    image: nginx\n</pre>\n<h3>How does this work?</h3>\n<p>A request can be set through the property <code>resources.requests.cpu</code> of a container. Alternatively, a default request for a namespace can\nbe set with <code>LimitRange</code> through the property <code>spec.limits[].defaultRequest.cpu</code>.</p>\n<h3>Documentation</h3>\n<ul>\n  <li> Kubernetes Documentation - <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/\">Configure\n  Default CPU Requests and Limits for a Namespace</a> </li>\n</ul>\n<h3>Articles &amp; blog posts</h3>\n<ul>\n  <li> Google Cloud Blog - <a\n  href=\"https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-resource-requests-and-limits\">Kubernetes best\n  practices: Resource requests and limits</a> </li>\n</ul>\n<p>A CPU request is a configuration that sets the guaranteed amount of CPU cores that a container will be able to use. It is part of the resource\nmanagement functionality of Kubernetes, which allows for the control and allocation of computational resources to containers.</p>\n<p>When a CPU request is set for a container, Kubernetes will only schedule it on a node that can give it that resource, thereby guaranteeing that the\ncontainer can use the specified requested CPU cores.</p>",
            "rule_severity": "MAJOR",
            "rule_type": "CODE_SMELL",
            "rule_status": "READY"
        }
    }    
]